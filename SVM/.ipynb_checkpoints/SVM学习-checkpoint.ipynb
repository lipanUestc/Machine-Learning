{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.csdn.net/luoshixian099/article/details/51073885  \n",
    "https://www.jianshu.com/c/bda3e98b8294?utm_source=desktop&utm_medium=notes-included-collection\n",
    "  \n",
    "# 1、最大间隔线性分类器\n",
    "数据点用列向量X表示，X={$x_1$,$x_2$,...,$x_n$},X有n个属性。  \n",
    "在X的n维空间中再加一维y，则是一个n+1维空间。 $W^TX+b=0$则表示在这n+1维空间中的一个n维超平面。  \n",
    "容易看出$W^T$是该平面的法向量。  \n",
    "假设所有数据是线性可分的，即所有数据一定落在超平面两侧。且$f(x)=W^TX+b$为正时，其标签y=1；为负时，其y=-1。所以$y*f(x)$一定为正。  \n",
    "  \n",
    "对于任意点X，其到超平面的**几何距离**$=\\frac{W^TX+b}{\\left \\| W \\right \\|}$，几何距离就是在坐标系中X到超平面的实际距离。  \n",
    "**函数距离**$=W^TX+b$,等于是几何距离乘以法向量的模$\\left \\| W \\right \\|$。函数距离的值本身没意义，但是两个点到同一个超平面的函数距离大小比较的相对值，可以说明他们几何距离的大小。    \n",
    "\n",
    "我们现在的目标是找到一个最鲁棒的划分平面，能够最好的将样本二分类。这里我们明确我们所指的最鲁棒为：所有超平面支持向量到该超平面几何距离的最大距离，这里有极小和极大两重含义：一是对于一个W、b而言，也即对于一个确定超平面而言，我们要找的是其几何距离中最小的点，因为训练集D中每个点到该超平面都有一个几何距离，但我们只找最小的，也就是找支持向量到超平面的几何距离，而支持向量以外的点我们统统不考虑，从这个优化目标就可以看出支持向量机的特性；二是对于不同的超平面，我们要找到所有最小几何距离中的最大值，因为对每个超平面而言，其支持向量可能是训练集D中不同的点，其支持向量到超平面的几何距离也不同。我们只能保证每个超平面的支持向量到平面的几何距离是所有点中最小的，但现在我们要找的是所有超平面中最小几何距离的最大值。\n",
    "  \n",
    "根据上述定义，我们写出要优化的几何距离的数学表达式：$\\frac{W^TX+b}{\\left \\| W \\right \\|}$，注意这就是点到平面的距离公式，算出的就是几何距离。这里要注意的要点是：我们算出的这个值就是一个特定W、b的超平面的支持向量到该平面的几何距离，就是支持向量而不是其他某个样本点到超平面的距离，虽然我们不知道这个超平面的支持向量具体是哪个点，但我们可以通过如下约束来保证：$y_i(W^TX_i+b)\\geq1$，我们稍等再讲如何保证。这里我们先来讲一讲为什么上边的数学表达式可以写成$\\frac{1}{\\left \\| W \\right \\|}$的形式。首先对于几何距离表达式$\\frac{W^TX+b}{\\left \\| W \\right \\|}$分子分母同乘以$\\frac{1}{k}$原式值不变，也即同时对W、b放缩k倍不影响点X到超平面W、b的几何距离。我们注意到这个算式算的是一个特定超平面对于一个特定训练数据集D的最小几何距离，所以W、b、X都是定值。由此可知算式分子部分$(W^TX+b)$就是一个常数k，所以分子分母同乘以$\\frac{1}{k}$也即将W、b同时放缩为原来的$\\frac{1}{k}$倍，原式一定可以写成$\\frac{1}{\\left \\| W \\right \\|}$分子是1的形式且保证算出的几何距离不变。这里有必要阐明一点打消大家的顾虑：就是W和b变化会不会使得不同超平面之间发生冲突，这里我们说明：$\\frac{b}{\\left \\| W \\right \\|}$的不同取值唯一确定了一种不同超平面，例如(1.5W,1.5b)(2W,2b)(kW,kb)等一簇表示的是一种超平面情况,或许两个不同超平面的W取值可能相同，但是这时b或X绝对不同，假定两个不同超平面最后都可以化为$\\frac{1}{\\left \\| W \\right \\|}$的形式，且W一样，则说明要么是不同超平面的支持向量不一样，恰巧取得了相同的最小几何距离，要么两个不同超平面针对的又是相同的支持向量其最小几何距离一定不等。\n",
    "   \n",
    "最后再来说明一下约束不等式如何保证了我们算式中的X一定是支持向量，由之前的证明可知$\\frac{1}{\\left \\| W \\right \\|}$就是某个超平面的支持向量的几何距离，显然穿过该支持向量的平行线为$W^TX+b=1$，也即该支持向量到超平面的函数距离为1。我们前边说过函数距离的大小可以表明几何距离的相对大小，且我们已知支持向量的函数距离为1 且 假设这个点就是支持向量，那么其他所有点都应该在支持向量之后，也就是其函数距离都应该大于等于1 $y_i(W^TX_i+b)\\geq 1\\quad$\n",
    "    \n",
    "综上，可将原始问题表述为$max \\quad \\frac{1}{\\left \\| W \\right \\|} \\quad s.t. \\quad y_i(W^TX_i+b)\\geq 1\\quad$ ,   \n",
    "取倒数然后平方：$min_{W,b} \\quad \\frac{1}{2}\\left \\| W \\right \\|^2 \\quad s.t. \\quad y_i(W^TX_i+b)\\geq 1\\quad (1)$ \n",
    "\n",
    "# 2、拉格朗日乘子法、KKT条件\n",
    "三种优化问题：  \n",
    "1、无约束直接求导等于0 $\\quad$   \n",
    "2、等式约束,等高线法线和约束超平面法线共线 $\\quad$   \n",
    "3、不等式约束，依旧用拉格朗日函数，但是求出的最优解只是候选值，要用KKT条件去验证。  \n",
    "  \n",
    "观察原始问题最终公式是一个不等式约束的优化问题，先写出其拉格朗日函数：  \n",
    "$$L(W,b,\\alpha )=\\frac{1}{2}\\left \\| W \\right \\|^2 + \\sum_{i=1}^{m}\\alpha (1-y(W^TX+b))\\quad(2)$$\n",
    "$min_{W,b}max_{\\alpha}\\quad L(W,b,\\alpha) \\quad s.t.\\quad \\alpha \\geq 0$就等价于(1)。这里还有一个隐藏约束，就是最后求得候选值还要满足KKT条件。  \n",
    "   \n",
    "这里为证明拉格朗日函数形式(2)与(1)等价，可以验证其求得的最优解是否一样.(1)的最优解是g(x)<=0范围内的f(x)最优解最小值。(2)的最优解可以分为两种情况讨论，(2)式本身解就有两种，因为由约束条件$\\alpha\\geq0$,当g(x)<=0在x可行域内时，L函数的后半部分一定小于等于0，所以此时其L的最大值就是f(x)在g(x)<=0内的最大值。当g(x)>0不在可行域，又因为α>=0且α为变量,所以max L一定是无穷大。所以再对max L取最小值就是f(x)在可行域g(x)<=0内最大值的最小值。\n",
    "\n",
    "# 3、对偶问题、强弱对偶性\n",
    "为方便求解，将原始问题（以W，b为变量的优化问题）$min_{W,b}max_{\\alpha}\\quad L(W,b,\\alpha) \\quad$转换为以$\\alpha$为变量的优化问题$$max_{\\alpha}min_{W,b}\\quad L(W,b,\\alpha) \\quad=max_{\\alpha}min_{W,b}\\quad \\frac{1}{2}\\left \\| W \\right \\|^2 + \\sum_{i=1}^{m}\\alpha (1-y(W^TX+b))\\quad$$。因为对偶问题最外层是$\\alpha$的函数且内层也是求极值问题，考虑先求偏导为0找到驻点，同时通过偏导为0条件，消掉W、b将对偶问题变成只含变量$\\alpha$的新问题。\n",
    "$$\\frac{\\partial L}{\\partial W}=\\left \\| W \\right \\|-\\sum_{i=1}^m\\alpha_iy_ix_i=0 \\quad->\\quad\\left \\| W \\right \\|=\\sum_{i=1}^m\\alpha_iy_ix_i$$\n",
    "$$\\frac{\\partial L}{\\partial b}=\\sum_{i=1}^m\\alpha_iy_i=0$$\n",
    "将上两式带入对偶函数得：\n",
    "$$max_{\\alpha} \\quad \\sum_{i=1}^m\\alpha_i-\\frac{1}{2}\\sum_{i=1}^m\\sum_{j=1}^m\\alpha_i\\alpha_jy_iy_jx_ix_j \\quad s.t. \\quad \\sum_{i=1}^m \\alpha_i y_i =0 且 \\alpha_i \\geq0 \\quad(3)$$\n",
    "注意这里的$\\alpha$和y是数，x则是样本向量，向量每个值是一个属性。  \n",
    "另外要注意W没有下标i，因此可作为常数提出求和符号，另外向量W和向量X的位置不能变。\n",
    "# 4、SMO或二次规划\n",
    "https://blog.csdn.net/luoshixian099/article/details/51227754  \n",
    "由SMO方法求解(3)公式，得到一组最优的$\\alpha$(α1，α2...αm)，带回W对L偏导为0的公式即可解得W,这样做的原因是求得的是该式的极值点:\n",
    "$$\\quad\\left \\| W \\right \\|=\\sum_{i=1}^m\\alpha_iy_ix_i$$\n",
    "接着求b，首先明确由KKT条件约束，可行解必满足$\\alpha_i=0 或者 y_i(W^T x_i+b)=1$,且不可能所有α都为0，否则上式算出W等于0表示超平面不存在，故一定存在分量αi不等于0，而该分量对应的样本xi就是支持向量，其函数距离必满足$y_i(W^T x_i+b)=1$,将x、y、W带入即可解得b。最终解得模型为：\n",
    "$$f(x)=W^T X+b=\\sum_{i=1}^m\\alpha_i y_i X_i^T X+b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
